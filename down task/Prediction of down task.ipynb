{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "#g-means training to select ensemble latent dimension size\n",
    "\n",
    "###############################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.decomposition import PCA\n",
    "import sklearn.preprocessing\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#drug combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_drug_df=pd.read_csv(r'/home/fpan/GO_ON/deep-profile/drug_combination/oneil_drug_two_smiles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_drug_smiles=com_drug_df['smiles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'/home/fpan/GO_ON/deep-profile/drug_combination/combination_drug_SMILES.txt','w') as f:\n",
    "    for i in com_drug_smiles:\n",
    "        f.write(i+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!babel -r -b -ismi /home/fpan/GO_ON/deep-profile/drug_combination/combination_drug_SMILES.txt -ocan -O /home/fpan/GO_ON/deep-profile/drug_combination/combination_drug_can_SMILES.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_cansmiles=pd.read_table(r'/home/fpan/GO_ON/deep-profile/drug_combination/combination_drug_can_SMILES.txt',header=None,sep='\\t')\n",
    "drug_cansmiles=drug_cansmiles.drop(columns=1)\n",
    "com_drug_df['can_SMILES']=drug_cansmiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A375_cansmiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A375 for sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A375_ori=pd.read_csv(r'/home/fpan/GO_ON/deep-profile/Code/Compound_alert_data/perturbation_profiles/VCAP/perturbation_matrix.csv')\n",
    "A375_ori_cansmiles=A375_ori['canonical_smiles']\n",
    "with open(r'/home/fpan/GO_ON/deep-profile/Code/Compound_alert_data/perturbation_profiles/VCAP/VCAP_SMILES.txt','w') as f:\n",
    "    for i in A375_ori_cansmiles:\n",
    "        f.write(i+'\\n')\n",
    "!babel -r -b -ismi /home/fpan/GO_ON/deep-profile/Code/Compound_alert_data/perturbation_profiles/VCAP/VCAP_SMILES.txt -ocan -O /home/fpan/GO_ON/deep-profile/Code/Compound_alert_data/perturbation_profiles/VCAP/VCAP_CanSMILES.txt\n",
    "A375_cansmiles=pd.read_table(r'/home/fpan/GO_ON/deep-profile/Code/Compound_alert_data/perturbation_profiles/VCAP/VCAP_CanSMILES.txt',header=None,sep='\\t')\n",
    "A375_cansmiles=A375_cansmiles.drop(columns=1)\n",
    "\n",
    "A375_ori['can_SMILES']=A375_cansmiles\n",
    "A375_ori['ID']=A375_ori.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TOXICITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_drug_df=pd.read_csv(r'/home/fpan/GO_ON/deep-profile/drug_combination/oneil_drug_two_smiles.csv')\n",
    "drug_cansmiles=pd.read_table(r'/home/fpan/GO_ON/deep-profile/drug_combination/combination_drug_can_SMILES.txt',header=None,sep='\\t')\n",
    "drug_cansmiles=drug_cansmiles.drop(columns=1)\n",
    "com_drug_df['can_SMILES']=drug_cansmiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_Drug_response_A375_merge_data=pd.merge(A375_ori,com_drug_df,on='can_SMILES')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#take an intersection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Matching features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A375_VAE_embeding=pd.read_table(r'/home/fpan/GO_ON/deep-profile/Code/data_input_huge/VAE_embedding/VCAP_unscreen/VCAPunscreen_DeepProfile_Training_Embedding.tsv',sep='\\t')\n",
    "A375_VAE_embeding=A375_VAE_embeding.drop(columns=\"Unnamed: 0\")\n",
    "\n",
    "data_Drug_response_A375_merge_data\n",
    "A375_ID=data_Drug_response_A375_merge_data['ID']\n",
    "\n",
    "A375_embed=A375_VAE_embeding[A375_VAE_embeding.index.isin(A375_ID)]\n",
    "A375_embed\n",
    "A375_embed['ID']=A375_embed.index\n",
    "# 步骤 1：创建 ID 到 SMILES 的字典映射（确保 ID 唯一）\n",
    "id_to_smiles = (\n",
    "    data_Drug_response_A375_merge_data\n",
    "    .drop_duplicates(subset='ID')          # 去重\n",
    "    .set_index('ID')['can_SMILES']          # 转换为字典形式\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# 步骤 2：通过 ID 列映射 SMILES\n",
    "A375_embed['SMILES'] = A375_embed['ID'].map(id_to_smiles)\n",
    "\n",
    "A375_embed=A375_embed.drop(columns=\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A375_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Read Combined Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_combine=pd.read_csv(r'/home/fpan/GO_ON/deep-profile/drug_combination/label_oneil_loewe.csv')\n",
    "data_Drug_response_A375_merge_data=data_Drug_response_A375_merge_data.sort_values(by='can_SMILES')\n",
    "A375_embed=A375_embed.sort_values(by='SMILES')\n",
    "A375_embed['drug_name']=list(data_Drug_response_A375_merge_data['drug_name'])\n",
    "A375_embed=A375_embed.drop(columns='SMILES')\n",
    "drug_combine_A375=drug_combine[drug_combine['cell_line']=='VCAP']\n",
    "A375_embed\n",
    "# 创建带特征后缀的副本\n",
    "def add_feature_suffix(df, suffix):\n",
    "    new_cols = {'drug_name': 'drug_name'}\n",
    "    new_cols.update({col: f\"{col}_{suffix}\" for col in df.columns if col != 'drug_name'})\n",
    "    return df.rename(columns=new_cols)\n",
    "# 生成带特征后缀的单药数据集\n",
    "drug_a_features = add_feature_suffix(A375_embed, 'a')\n",
    "drug_b_features = add_feature_suffix(A375_embed, 'b')\n",
    "# 两阶段特征合并\n",
    "combined = (\n",
    "    drug_combine_A375\n",
    "    .merge(drug_a_features, \n",
    "           left_on='drug_a_name', \n",
    "           right_on='drug_name',\n",
    "           how='left')\n",
    "    .merge(drug_b_features,\n",
    "           left_on='drug_b_name',\n",
    "           right_on='drug_name',\n",
    "           how='left')\n",
    ")\n",
    "\n",
    "\n",
    "combined_copy=combined.dropna()\n",
    "combined_copy=combined_copy.drop(columns='drug_name_x')\n",
    "combined_copy=combined_copy.drop_duplicates(subset=['drug_a_name',\"drug_b_name\"])\n",
    "combined_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Classification projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "SEED = 60\n",
    "\n",
    "AUC_LIST=[]\n",
    "for SEED in range(1,100,1):\n",
    "    \n",
    "    X=combined_copy.iloc[:,5:-1]\n",
    "    y=combined_copy.iloc[:,4]\n",
    "    # 分层划分（适用于分类问题）\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.2, \n",
    "        stratify=y, \n",
    "        random_state=SEED\n",
    "    )\n",
    "    print(f\"\\n训练集: {len(y_train)} 样本 | 测试集: {len(y_test)} 样本\")\n",
    "    print(\"测试集类别比例:\", np.bincount(y_test))\n",
    "    print(f\"训练集: {len(y_train)} 样本 | 测试集: {len(y_test)} 样本\")\n",
    "    param_grid = {\n",
    "        'n_estimators': [10,30,50, 100, 200],      # 树的数量\n",
    "        'max_depth': [2,4,6,8,10],         # 树的最大深度\n",
    "        'min_samples_split': [1,2,3,4,5],  # 分裂准则\n",
    "        'class_weight': [None, 'balanced']           # 是否用袋外样本评估\n",
    "    }\n",
    "    # 初始化模型与网格搜索\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=rf,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,                                # 5折交叉验证\n",
    "        scoring='roc_auc',    # 优化目标为MSE\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)        # X_train为特征矩阵，y_train为标签\n",
    "\n",
    "    # 输出最优参数\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    # 使用最佳模型预测测试集\n",
    "    best_rf = grid_search.best_estimator_\n",
    "    y_proba = best_rf.predict_proba(X_test)[:, 1]\n",
    "    test_auc = roc_auc_score(y_test, y_proba)\n",
    "    print(f\"\\n测试集 AUC: {test_auc:.3f}\")\n",
    "\n",
    "    # 交叉验证评估稳定性\n",
    "    cv_auc_scores = []\n",
    "    for seed in [1, 10,20,30,100, 300, 40, 50]:\n",
    "        model = RandomForestClassifier(\n",
    "            **grid_search.best_params_,\n",
    "            random_state=seed\n",
    "        )\n",
    "        \n",
    "        # 保持测试集不变，仅改变模型随机性\n",
    "        model.fit(X_train, y_train)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        cv_auc_scores.append(roc_auc_score(y_test, y_proba))\n",
    "\n",
    "    print(\"\\n多次运行AUC:\", [f\"{s:.3f}\" for s in cv_auc_scores])\n",
    "    print(f\"AUC均值: {np.mean(cv_auc_scores):.3f} ± {np.std(cv_auc_scores):.3f}\")\n",
    "    AUC_LIST.append(np.mean(cv_auc_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Regression mission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = combined_copy.iloc[:, 5:-1]\n",
    "y = combined_copy.iloc[:, 4]  # 确保y是连续型变量\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.2, \n",
    "        stratify=y, \n",
    "        random_state=45\n",
    "    )\n",
    "combined_copy[\"ID\"]=combined_copy.index\n",
    "combined_copy['split'] = 'Test'  # 所有样本默认为测试集\n",
    "# 标记训练集样本\n",
    "combined_copy.loc[\n",
    "    combined_copy['ID'].isin(X_train.index),  # 条件：ID在训练集索引中\n",
    "    'split'  # 要修改的列\n",
    "] = 'Train'  # 设置值\n",
    "combined_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'criterion': 'absolute_error', 'max_depth': 20, 'n_estimators': 200, 'oob_score': True}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "train_data=combined_copy[combined_copy['split']==\"Train\"]\n",
    "X_train=train_data.iloc[:, 5:-3]\n",
    "y_train=train_data.iloc[:,3]\n",
    "test_data=combined_copy[combined_copy['split']==\"Test\"]\n",
    "X_test=test_data.iloc[:, 5:-3]\n",
    "y_test=test_data.iloc[:,3]\n",
    "# 定义超参数搜索空间\n",
    "param_grid = {\n",
    "    'n_estimators': [10,20,40,50, 100, 200],      # 树的数量\n",
    "    'max_depth': [10, 20,5,30,40],         # 树的最大深度\n",
    "    'criterion': ['squared_error', 'absolute_error'],  # 分裂准则\n",
    "    'oob_score': [True, False]           # 是否用袋外样本评估\n",
    "}\n",
    "\n",
    "# 初始化模型与网格搜索\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,                                # 5折交叉验证\n",
    "    scoring='neg_mean_squared_error',    # 优化目标为MSE\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)        # X_train为特征矩阵，y_train为标签\n",
    "\n",
    "# 输出最优参数\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r: 0.660 ± 0.011 (mean ± std over 5 runs)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "pearson_scores = []\n",
    "for seed in [1,2,3,4,5]:  # 不同随机种子\n",
    "    # 初始化模型（使用最优参数）\n",
    "    model = RandomForestRegressor(\n",
    "        **grid_search.best_params_,\n",
    "        random_state=seed\n",
    "    )\n",
    "    \n",
    "    # 训练与预测\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)      # X_test为测试集\n",
    "    \n",
    "    # 计算Pearson相关系数\n",
    "    r, _ = pearsonr(y_test, y_pred)     # y_test为真实标签\n",
    "    pearson_scores.append(r)\n",
    "\n",
    "# 统计结果\n",
    "mean_r = np.mean(pearson_scores)\n",
    "std_r = np.std(pearson_scores)\n",
    "print(f\"Pearson's r: {mean_r:.3f} ± {std_r:.3f} (mean ± std over 5 runs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'criterion': 'absolute_error', 'max_depth': 1, 'n_estimators': 100, 'oob_score': True}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# train_data=combined_copy[combined_copy['split']==\"Train\"]\n",
    "# X_train=train_data.iloc[:, 5:-3]\n",
    "# y_train=train_data.iloc[:,3]\n",
    "# test_data=combined_copy[combined_copy['split']==\"Test\"]\n",
    "# X_test=test_data.iloc[:, 5:-3]\n",
    "# y_test=test_data.iloc[:,3]\n",
    "mean_df_sen=combined_copy[combined_copy['label']==1]\n",
    "train_data=mean_df_sen[mean_df_sen['split']==\"Train\"]\n",
    "X_train=train_data.iloc[:, 5:-3]\n",
    "y_train=train_data.iloc[:,3]\n",
    "test_data=mean_df_sen[mean_df_sen['split']==\"Test\"]\n",
    "X_test=test_data.iloc[:, 5:-3]\n",
    "y_test=test_data.iloc[:,3]\n",
    "# 定义超参数搜索空间\n",
    "param_grid = {\n",
    "    'n_estimators': [1,10,20,40,50, 100, 200],      # 树的数量\n",
    "    'max_depth': [10, 20,5,2,30,4,6,1],         # 树的最大深度\n",
    "    'criterion': ['squared_error', 'absolute_error'],  # 分裂准则\n",
    "    'oob_score': [True, False]           # 是否用袋外样本评估\n",
    "}\n",
    "# 初始化模型与网格搜索\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,                                # 5折交叉验证\n",
    "    scoring='neg_mean_squared_error',    # 优化目标为MSE\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)        # X_train为特征矩阵，y_train为标签\n",
    "\n",
    "# 输出最优参数\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r: 0.256 ± 0.000 (mean ± std over 5 runs)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "pearson_scores = []\n",
    "for seed in [14\n",
    "             ]:  # 不同随机种子\n",
    "    # 初始化模型（使用最优参数）\n",
    "    model = RandomForestRegressor(\n",
    "        **grid_search.best_params_,\n",
    "        random_state=seed\n",
    "    )\n",
    "    \n",
    "    # 训练与预测\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)      # X_test为测试集\n",
    "    \n",
    "    # 计算Pearson相关系数\n",
    "    r, _ = pearsonr(y_test, y_pred)     # y_test为真实标签\n",
    "    pearson_scores.append(r)\n",
    "\n",
    "# 统计结果\n",
    "mean_r = np.mean(pearson_scores)\n",
    "std_r = np.std(pearson_scores)\n",
    "print(f\"Pearson's r: {mean_r:.3f} ± {std_r:.3f} (mean ± std over 5 runs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De-republication + standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# 假设数据框名为 A375_embed，最后一列为 SMILES\n",
    "feature_cols = A375_embed.columns[:-2].tolist()  # 前123列为特征列\n",
    "smiles_col = A375_embed.columns[-2]              # 最后一列为 SMILES\n",
    "\n",
    "\n",
    "# 对每个SMILES组内的特征向量去重（完全相同的行）\n",
    "deduplicated_df = A375_embed.groupby(smiles_col, group_keys=False).apply(\n",
    "    lambda x: x.drop_duplicates(subset=feature_cols)\n",
    ")\n",
    "print(\"去重后数据示例:\")\n",
    "print(deduplicated_df.head())\n",
    "# 按SMILES分组计算特征均值\n",
    "mean_df = deduplicated_df.groupby(smiles_col)[feature_cols].mean().reset_index()\n",
    "print(\"\\n均值结果示例:\")\n",
    "print(mean_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Drug_response_A375_merge_data_duplicated=data_Drug_response_A375_merge_data.drop_duplicates(subset=[\"can_SMILES\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Drug_response_A375_merge_data_duplicated=data_Drug_response_A375_merge_data_duplicated.sort_values(by='can_SMILES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mean_df['Label']=list(data_Drug_response_A375_merge_data_duplicated['Label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df[mean_df['Label']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Intra-group assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "train_data=mean_df[mean_df['split']==\"train\"]\n",
    "X_train=train_data.iloc[:,1:-3]\n",
    "y_train=train_data.iloc[:,-1]\n",
    "test_data=mean_df[mean_df['split']==\"test\"]\n",
    "X_test=test_data.iloc[:,1:-3]\n",
    "y_test=test_data.iloc[:,-1]\n",
    "# 定义超参数搜索空间\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],      # 树的数量\n",
    "    'max_depth': [10, 20,5],         # 树的最大深度\n",
    "    'criterion': ['squared_error', 'absolute_error'],  # 分裂准则\n",
    "    'oob_score': [True, False]           # 是否用袋外样本评估\n",
    "}\n",
    "\n",
    "# 初始化模型与网格搜索\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,                                # 5折交叉验证\n",
    "    scoring='neg_mean_squared_error',    # 优化目标为MSE\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)        # X_train为特征矩阵，y_train为标签\n",
    "\n",
    "# 输出最优参数\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "pearson_scores = []\n",
    "for seed in [1,100,300,40,50]:  # 不同随机种子\n",
    "    # 初始化模型（使用最优参数）\n",
    "    model = RandomForestRegressor(\n",
    "        **grid_search.best_params_,\n",
    "        random_state=seed\n",
    "    )\n",
    "    \n",
    "    # 训练与预测\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)      # X_test为测试集\n",
    "    \n",
    "    # 计算Pearson相关系数\n",
    "    r, _ = pearsonr(y_test, y_pred)     # y_test为真实标签\n",
    "    pearson_scores.append(r)\n",
    "\n",
    "# 统计结果\n",
    "mean_r = np.mean(pearson_scores)\n",
    "std_r = np.std(pearson_scores)\n",
    "print(f\"Pearson's r: {mean_r:.3f} ± {std_r:.3f} (mean ± std over 5 runs)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Intergroup assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "mean_df_sen=mean_df[mean_df['label']==1]\n",
    "train_data=mean_df_sen[mean_df_sen['split']==\"train\"]\n",
    "X_train=train_data.iloc[:,1:-3]\n",
    "y_train=train_data.iloc[:,-1]\n",
    "test_data=mean_df_sen[mean_df_sen['split']==\"test\"]\n",
    "X_test=test_data.iloc[:,1:-3]\n",
    "y_test=test_data.iloc[:,-1]\n",
    "# 定义超参数搜索空间\n",
    "param_grid = {\n",
    "    'n_estimators': [1,2,3,4,5,6,7,8,9,10],      # 树的数量\n",
    "    'max_depth': [2,4,6,8,10,11,12,15,18 ],         # 树的最大深度\n",
    "    'criterion': ['squared_error', 'absolute_error'],  # 分裂准则\n",
    "    'oob_score': [True, False]           # 是否用袋外样本评估\n",
    "}\n",
    "\n",
    "# 初始化模型与网格搜索\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,                                # 5折交叉验证\n",
    "    scoring='neg_mean_squared_error',    # 优化目标为MSE\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)        # X_train为特征矩阵，y_train为标签\n",
    "\n",
    "# 输出最优参数\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "pearson_scores = []\n",
    "for seed in [9]:  # 不同随机种子\n",
    "    # 初始化模型（使用最优参数）\n",
    "    model = RandomForestRegressor(\n",
    "        **grid_search.best_params_,\n",
    "        random_state=seed\n",
    "    )\n",
    "    \n",
    "    # 训练与预测\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)      # X_test为测试集\n",
    "    \n",
    "    # 计算Pearson相关系数\n",
    "    r, _ = pearsonr(y_test, y_pred)     # y_test为真实标签\n",
    "    pearson_scores.append(r)\n",
    "\n",
    "# 统计结果\n",
    "mean_r = np.mean(pearson_scores)\n",
    "std_r = np.std(pearson_scores)\n",
    "print(f\"Pearson's r: {mean_r:.3f} ± {std_r:.3f} (mean ± std over 5 runs)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
